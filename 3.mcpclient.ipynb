{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25e0fcc",
   "metadata": {},
   "source": [
    "# Creating an MCP Client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e9840",
   "metadata": {},
   "source": [
    "In the previous section(2.mcpserver.ipynb), you created an MCP research server that exposes 2 tools. In this session, you will make the chatbot communicate to the server through an MCP client. This will make the chatbot MCP compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcff46",
   "metadata": {},
   "source": [
    "<img src=\"images/lesson_progression.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb923ced",
   "metadata": {},
   "source": [
    "## Back to the Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa864f",
   "metadata": {},
   "source": [
    "Here are the main code parts (`process_query`, `chat_loop`) from the chatbot example of 1.chatbot.ipynb. Notice that the burden of tool definitions and execution is now shifted onto the MCP server, so the chatbot logic only contains code related to processing the user queries and to keeping the chat loop running until the user types `quit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aafe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()\n",
    "anthropic = anthropic.Anthropic()\n",
    "\n",
    "def process_query(query):\n",
    "    messages = [{'role':'user', 'content':query}]\n",
    "    response = anthropic.messages.create(max_tokens = 2024,\n",
    "                                  model = 'claude-3-7-sonnet-20250219', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages)\n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "        for content in response.content:\n",
    "            if content.type =='text':\n",
    "                print(content.text)\n",
    "                assistant_content.append(content)\n",
    "                if(len(response.content)==1):\n",
    "                    process_query= False\n",
    "            elif content.type == 'tool_use':\n",
    "                assistant_content.append(content)\n",
    "                messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                # Call a tool\n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append({\"role\": \"user\", \n",
    "                                  \"content\": [\n",
    "                                      {\n",
    "                                          \"type\": \"tool_result\",\n",
    "                                          \"tool_use_id\":tool_id,\n",
    "                                          \"content\": result\n",
    "                                      }\n",
    "                                  ]\n",
    "                                })\n",
    "                response = anthropic.messages.create(max_tokens = 2024,\n",
    "                                  model = 'claude-3-7-sonnet-20250219', \n",
    "                                  tools = tools,\n",
    "                                  messages = messages) \n",
    "                \n",
    "                if(len(response.content)==1 and response.content[0].type == \"text\"):\n",
    "                    print(response.content[0].text)\n",
    "                    process_query= False\n",
    "\n",
    "\n",
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d4726",
   "metadata": {},
   "source": [
    "## Building your MCP Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b36531",
   "metadata": {},
   "source": [
    "Now you will take the functions `process_query` and `chat_loop` and wrap them in a `MCP_ChatBot` class. To enable the chatbot to communicate to the server, you will add a method that connects to the server through an MCP client, which follows the structure given in this reference code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reference Code\n",
    "``` python\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uv\",  # Executable\n",
    "    args=[\"run example_server.py\"],  # Command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    # Launch the server as a subprocess & returns the read and write streams\n",
    "    # read: the stream that the client will use to read msgs from the server\n",
    "    # write: the stream that client will use to write msgs to the server\n",
    "    async with stdio_client(server_params) as (read, write): \n",
    "        # the client session is used to initiate the connection \n",
    "        # and send requests to server \n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection (1:1 connection with the server)\n",
    "            await session.initialize()\n",
    "\n",
    "            # List available tools\n",
    "            tools = await session.list_tools()\n",
    "\n",
    "            # will call the chat_loop here\n",
    "            # ....\n",
    "            \n",
    "            # Call a tool: this will be in the process_query method\n",
    "            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run())\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5627f53",
   "metadata": {},
   "source": [
    "### Adding MCP Client to the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e941d",
   "metadata": {},
   "source": [
    "The MCP_ChatBot class consists of the methods:\n",
    "- `process_query`\n",
    "- `chat_loop`\n",
    "- `connect_to_server_and_run`\n",
    "  \n",
    "and has the following attributes:\n",
    "- session (of type ClientSession)\n",
    "- anthropic: Anthropic                           \n",
    "- available_tools\n",
    "\n",
    "In `connect_to_server_and_run`, the client launches the server and requests the list of tools that the server provides (through the client session). The tool definitions are stored in the variable `available_tools` and are passed in to the LLM in `process_query`.\n",
    "\n",
    "<img src=\"images/tools_discovery.png\" width=\"400\">\n",
    "\n",
    "\n",
    "In `process_query`, when the LLM decides it requires a tool to be executed, the client session sends to the server the tool call request. The returned response is passed in to the LLM. \n",
    "\n",
    "<img src=\"images/tool_invocation.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad387917",
   "metadata": {},
   "source": [
    "Here're the `mcp_chatbot` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9a0f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2.1.mcp_project/mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2.1.mcp_project/mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.anthropic = Anthropic()\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "                                      model = 'claude-3-7-sonnet-20250219', \n",
    "                                      tools = self.available_tools, # tools exposed to the LLM\n",
    "                                      messages = messages)\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            assistant_content = []\n",
    "            for content in response.content:\n",
    "                if content.type =='text':\n",
    "                    print(content.text)\n",
    "                    assistant_content.append(content)\n",
    "                    if(len(response.content) == 1):\n",
    "                        process_query= False\n",
    "                elif content.type == 'tool_use':\n",
    "                    assistant_content.append(content)\n",
    "                    messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                    tool_id = content.id\n",
    "                    tool_args = content.input\n",
    "                    tool_name = content.name\n",
    "    \n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Call a tool\n",
    "                    #result = execute_tool(tool_name, tool_args): not anymore needed\n",
    "                    # tool invocation through the client session\n",
    "                    result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append({\"role\": \"user\", \n",
    "                                      \"content\": [\n",
    "                                          {\n",
    "                                              \"type\": \"tool_result\",\n",
    "                                              \"tool_use_id\":tool_id,\n",
    "                                              \"content\": result.content\n",
    "                                          }\n",
    "                                      ]\n",
    "                                    })\n",
    "                    response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "                                      model = 'claude-3-7-sonnet-20250219', \n",
    "                                      tools = self.available_tools,\n",
    "                                      messages = messages) \n",
    "                    \n",
    "                    if(len(response.content) == 1 and response.content[0].type == \"text\"):\n",
    "                        print(response.content[0].text)\n",
    "                        process_query= False\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\",  # Executable\n",
    "            args=[\"run\", \"research_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection\n",
    "                await session.initialize()\n",
    "    \n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "                \n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "                \n",
    "                self.available_tools = [{\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                } for tool in response.tools]\n",
    "    \n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc4785",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2de5b",
   "metadata": {},
   "source": [
    "**Terminal Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb2515",
   "metadata": {},
   "source": [
    "- To open the terminal, run the cell below.\n",
    "- Navigate to the `2.1.mcp_project` directory:\n",
    "    - `cd 2.1./mcp_project`\n",
    "- Activate the virtual environment:\n",
    "    - `source .venv/bin/activate`\n",
    "- Install the additional dependencies:\n",
    "    - `uv add anthropic python-dotenv nest_asyncio`\n",
    "- Run the chatbot:\n",
    "    - `uv run mcp_chatbot.py`\n",
    "- To exit the chatbot, type `quit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb0e76",
   "metadata": {},
   "source": [
    "**Questions for search on the terminal**\n",
    "- Hi\n",
    "- can you search for papers around physics and find just two of them for me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
